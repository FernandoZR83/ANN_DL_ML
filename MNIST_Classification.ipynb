{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_Classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FernandoZR83/ANN_DL_ML/blob/master/MNIST_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtaO6jVHxwF_",
        "colab_type": "text"
      },
      "source": [
        "# Deep Neural Network for MNIST Classification\n",
        "\n",
        "We'll apply all the knowledge from the lectures in this section to write a deep neural network. The problem we've chosen is referred to as the \"Hello World\" for machine learning because for most students it is their first example. The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as covolutional networks. The dataset provides 28x28 images of handwritten digits (1 per image) and the goal is to write an algorithm that detects which digit is written. Since there are only 10 digits, this is a classification problem with 10 classes. In order to exemplify what we've talked about in this section, we will build a network with 2 hidden layers between inputs and outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzCtLI4Axug3",
        "colab_type": "text"
      },
      "source": [
        "## Import relevant libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V01ZT3dFs3D0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install -q tensorflow-gpu==2.0.0-beta1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-gjVOK0scnO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "1822e899-b1f5-448c-a19a-f7d0c52ffe9c"
      },
      "source": [
        "!pip show tensorflow"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 1.14.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: numpy, termcolor, protobuf, gast, tensorflow-estimator, six, wrapt, keras-applications, grpcio, absl-py, keras-preprocessing, google-pasta, astor, tensorboard, wheel\n",
            "Required-by: stable-baselines, magenta, fancyimpute\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWs9mG7KyJ4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urJ9F11_3UaV",
        "colab_type": "text"
      },
      "source": [
        " TensorFLow includes a data provider for MNIST that we'll use.\n",
        " This function automatically downloads the MNIST dataset to the chosen directory. \n",
        " The dataset is already split into training, validation, and test subsets. \n",
        " Furthermore, it preprocess it into a particularly simple and useful format.\n",
        " Every 28x28 image is flattened into a vector of length 28x28=784, where every value\n",
        " corresponds to the intensity of the color of the corresponding pixel.\n",
        " The samples are grayscale (but standardized from 0 to 1), so a value close to 0 is almost white and a value close to\n",
        " 1 is almost purely black. This representation (flattening the image row by row into\n",
        " a vector) is slightly naive but as you'll see it works surprisingly well.\n",
        " Since this is a classification problem, our targets are categorical.\n",
        " Recall from the lecture on that topic that one way to deal with that is to use one-hot encoding.\n",
        " With it, the target for each individual sample is a vector of length 10\n",
        " which has nine 0s and a single 1 at the position which corresponds to the correct answer.\n",
        " For instance, if the true answer is \"1\", the target will be [0,0,0,1,0,0,0,0,0,0] (counting from 0).\n",
        " Have in mind that the very first time you execute this command it might take a little while to run\n",
        " because it has to download the whole dataset. Following commands only extract it so \n",
        " they're faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvJTRkWe3TXB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "21472550-1177-4153-aa52-14b5adf3ffce"
      },
      "source": [
        "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0729 17:08:16.728907 139760209393536 dataset_builder.py:397] Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LqhvY1v3I4V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50078c21-a7eb-47d8-f131-6fe3c376aa36"
      },
      "source": [
        "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
        "#Now we take the validation dataset becuase tensorflow does not contain validation data set\n",
        "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
        "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
        "num_test_samples = 0.1 * mnist_info.splits['test'].num_examples\n",
        "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
        "\n",
        "def scale(image, label):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image /= 255. \n",
        "  #Dot at the end indicates we want it to be flat even if divided by 255\n",
        "  return image, label\n",
        "#dataset.map(*function*) applies a custom transformation to a given dataset , it takes as input a funciotn which determines the transformation\n",
        "scaled_train_and_validation_data = mnist_train.map(scale)\n",
        "test_data = mnist_test.map(scale)\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "shuffled_train_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
        "\n",
        "validation_data = shuffled_train_validation_data.take(num_validation_samples)\n",
        "train_data = shuffled_train_validation_data.skip(num_validation_samples)\n",
        "\n",
        "#Hyperparameter\n",
        "batch_size = 100\n",
        "\n",
        "#dataset.batch(BATCH_SIZE) a method that combines the consecutive elements of a set  into batches\n",
        "\n",
        "train_data = train_data.batch(batch_size)\n",
        "validation_data = validation_data.batch(num_validation_samples)\n",
        "test_data = test_data.batch(num_test_samples)\n",
        "\n",
        "validation_inputs, validation_targets = next(iter(validation_data))\n",
        "\n",
        "validation_inputs\n",
        "validation_targets"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'IteratorGetNext_2:1' shape=(?,) dtype=int64>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8wDINMoHFYP",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv52A6k-HHpv",
        "colab_type": "text"
      },
      "source": [
        "Outline the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzVsm2sTHJO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = 784\n",
        "output_size = 10 #one per digit\n",
        "hidden_layer_size = 50 #50 nodes per layer\n",
        "#tf.keras.sequential is a function used to \"stack layers\"\n",
        "#Our model's name is model\n",
        "model = tf.keras.Sequential([\n",
        "    #we need to flat images to get them a vector\n",
        "    #First line in sequential function is used to delcare our input layer\n",
        "    tf.keras.layers.Flatten(input_shape = (28,28,1)),\n",
        "    #tf.keras.layers.Dense(output_size) takes the inputs, provided to the model and calculates the dot product of the\n",
        "    #inputs and the weights and adds the bias.\n",
        "    #This is also where we can apply an activation function\n",
        "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "    #We create the second hidden layer the same way\n",
        "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "    #Output layer\n",
        "    tf.keras.layers.Dense(output_size, activation = 'softmax')\n",
        "\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k2BgFKzgrPF",
        "colab_type": "text"
      },
      "source": [
        "Choose the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugDYSQBKgkXZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "657975e8-f734-4ccd-db08-e67edf2ffb09"
      },
      "source": [
        "#model.compile(optimizer, loss, metrics) configures the model for training\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_4 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 50)                39250     \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 10)                510       \n",
            "=================================================================\n",
            "Total params: 42,310\n",
            "Trainable params: 42,310\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG7AFIlgg5Es",
        "colab_type": "text"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGJKNC8Dg6JU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "0d32980b-f884-49f1-c796-4e9f6c7ff3e2"
      },
      "source": [
        "#Choose number of epochs\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "model.fit(train_data, epochs = NUM_EPOCHS, verbose=1)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "540/540 [==============================] - 17s 32ms/step - loss: 0.4137 - acc: 0.8835\n",
            "Epoch 2/5\n",
            "540/540 [==============================] - 16s 30ms/step - loss: 0.1847 - acc: 0.9453\n",
            "Epoch 3/5\n",
            "540/540 [==============================] - 16s 30ms/step - loss: 0.1401 - acc: 0.9583\n",
            "Epoch 4/5\n",
            "540/540 [==============================] - 16s 29ms/step - loss: 0.1134 - acc: 0.9661\n",
            "Epoch 5/5\n",
            "540/540 [==============================] - 16s 30ms/step - loss: 0.0952 - acc: 0.9714\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1bbdcf7eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj-4BOf8IflF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "315e8d8c-ddf1-489d-a9ce-4c92ea1c99fb"
      },
      "source": [
        "#to store in variables test loss and test accuracy, we write:\n",
        "test_loss, test_accuracy = model.evaluate(test_data)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     10/Unknown - 3s 255ms/step - loss: 0.1194 - acc: 0.9636"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CotuEXyC_2kJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81b02b09-92b4-4526-f0c2-eedbff7f75fd"
      },
      "source": [
        "print('Test loss: {0:.2f}, Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.12, Test accuracy: 96.36%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}